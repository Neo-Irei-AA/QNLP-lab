{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e503c453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neoia\\anaconda3\\envs\\lambeq_env2\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\neoia\\anaconda3\\envs\\lambeq_env2\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from lambeq.backend.drawing import draw\n",
    "from lambeq.backend.grammar import Cup, Id, Ty, Word\n",
    "from lambeq import AtomicType, IQPAnsatz, NumpyModel, BinaryCrossEntropyLoss, QuantumTrainer, SPSAOptimizer, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "n, s = Ty('n'), Ty('s')\n",
    "# nlp = spacy.load(\"ja_core_web_sm\")\n",
    "nlp = spacy.load(\"ja_ginza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de29db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット読み込み\n",
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = int(line[0])\n",
    "            # labels.append([[0, t],[0, 1-t]]) # 2×2行列の形で格納するならこっち\n",
    "            labels.append([1-t, t]) # 1×2行列の形\n",
    "            sentences.append(line[1:].strip())\n",
    "            \n",
    "    return np.array(labels), sentences\n",
    "\n",
    "train_labels, train_sentences = read_data('C:/Users/neoia/研究/data/mc_train_data_jp.txt')\n",
    "dev_labels, dev_sentences = read_data('C:/Users/neoia/研究/data/mc_dev_data_jp.txt')\n",
    "test_labels, test_sentences = read_data('C:/Users/neoia/研究/data/mc_test_data_jp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b501decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train ---\n",
      "最大トークン数: 11, 文: 熟練した男はソースを準備します。\n",
      "最小トークン数: 7, 文: 女性は夕食を焼きます。\n",
      "\n",
      "--- dev ---\n",
      "最大トークン数: 11, 文: 熟練した人が申請を準備します。\n",
      "最小トークン数: 7, 文: 男はソースを焼きます。\n",
      "\n",
      "--- test ---\n",
      "最大トークン数: 12, 文: 熟練した女性は夕食を作っています。\n",
      "最小トークン数: 7, 文: 女性は食事を焼きます。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# トークン数をカウントして最大・最小の文を探す関数\n",
    "def print_max_min_tokens(sentences, name=\"dataset\"):\n",
    "    token_counts = [len(nlp(sent)) for sent in sentences]\n",
    "    \n",
    "    max_idx = np.argmax(token_counts)\n",
    "    min_idx = np.argmin(token_counts)\n",
    "    \n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"最大トークン数: {token_counts[max_idx]}, 文: {sentences[max_idx]}\")\n",
    "    print(f\"最小トークン数: {token_counts[min_idx]}, 文: {sentences[min_idx]}\\n\")\n",
    "\n",
    "# 各データセットで確認\n",
    "print_max_min_tokens(train_sentences, \"train\")\n",
    "print_max_min_tokens(dev_sentences, \"dev\")\n",
    "print_max_min_tokens(test_sentences, \"test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lambeq_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
